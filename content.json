{"meta":{"title":"Hexo","subtitle":"","description":"","author":"lulu-cloud","url":"https://lulu-cloud.github.io","root":"/"},"pages":[],"posts":[{"title":"疯狂的横向：开题期间","slug":"开题、科研与横向","date":"2023-11-23T13:42:14.000Z","updated":"2023-11-24T13:38:35.077Z","comments":true,"path":"2023/11/23/开题、科研与横向/","link":"","permalink":"https://lulu-cloud.github.io/2023/11/23/%E5%BC%80%E9%A2%98%E3%80%81%E7%A7%91%E7%A0%94%E4%B8%8E%E6%A8%AA%E5%90%91/","excerpt":"","text":"狠狠滴看领域内的论文，编开题报告！！ 横向横向横向，每次会议的主题。无语辣。 听首歌，只想那种积极的，听得我爽就好，邓紫棋的《再见》","categories":[{"name":"硕士生活","slug":"硕士生活","permalink":"https://lulu-cloud.github.io/categories/%E7%A1%95%E5%A3%AB%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"生活随记","slug":"生活随记","permalink":"https://lulu-cloud.github.io/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/"}]},{"title":"加密流量分类-论文12: Seeing Traffic Paths_ Encrypted Traffic Classification With Path Signature Features","slug":"加密流量分类-论文12：Seeing Traffic Paths_ Encrypted Traffic Classification With Path Signature Features","date":"2023-01-19T13:27:14.000Z","updated":"2023-11-24T13:29:44.665Z","comments":true,"path":"2023/01/19/加密流量分类-论文12：Seeing Traffic Paths_ Encrypted Traffic Classification With Path Signature Features/","link":"","permalink":"https://lulu-cloud.github.io/2023/01/19/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%8712%EF%BC%9ASeeing%20Traffic%20Paths_%20Encrypted%20Traffic%20Classification%20With%20Path%20Signature%20Features/","excerpt":"","text":"0、摘要&emsp;&emsp;本文提出了一种新的带路径签名的加密流分类方法——ETC-PS。首先用会话数据包长度序列构造流量路径来表示客户端和服务器之间的交互。然后进行路径变换，展示其结构，获得不同的信息。最后计算出多尺度路径特征作为一种显著特征来训练传统的机器学习分类器，实现了高鲁棒精度和低训练开销。 1、模型方法 将路径签名应用与加密流量分类，只使用了流量的序列特征。 机器学习分类：要走数据预处理、特征工程、分类等过程，相交于深度学习方法多了特征工程 特征工程：从会话中客户端-服务器双向交互的流量特征构建了流量路径，再进行路径变换，作为分类器输入 1.1 路径签名定义与科普 1.1.1科普参考Path Signature笔记 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/335494125)) 1.1.2 路径签名的一些性质 唯一性：由路径签名可以完全确定一个路径，具有单调维度的路径(如时间)没有支路部分。因此，对于一个加密的流量报文长度序列(一维路径)，通常需要添加一个单调递增的维度(例如时间)，以保证路径签名可以等价地确定它 重新参数化不变性： 对于一个路径，采用不同的采样频率得到的路径表示会不同，但是路径签名不会变。对于特定类型应用程序生成的流量，分类结果应该不受不同重参数化的影响。有了这个特性，签名可以过滤掉同种流量的不同重参数化引起的变化，不会分为不同应用。 维数固定: 从整个路径中提取的路径特征的维数取决于截断级别，与路径长度无关。再短的路径其签名维数也是无限长，使用的时候取决于我们截断的维度是多少。 1.2 ETC-PS整体预览 收集报文序列，计算报文长度，生成报文长度序列，使用正负号表示报文序列的不同方向 进行路径构造 进行路径变换 进行路径签名特征提取得到特征 使用机器学习分类器进行分类 1.3 流量路径构造1.3.1 客户端与服务器的双向交互&emsp;&emsp;当客户端和服务器之间建立连接时，客户端通常会向服务器发送请求，请求服务器发送所需的资源。主要分为3个阶段 握手阶段：上行和下行报文在此阶段交替传输。对于同一种传输协议，在此阶段生成的报文序列非常相似，包括报文长度、数量、方向等。（有明显特征） 上行报文主导阶段：上行报文（C-&gt;S）主要在此阶段传输。通常将控制指令编码的上行报文发送给服务器，邀请服务器配合提高数据传输效率。 下行报文主导阶段：下行报文（S-&gt;C）主要在此阶段传输。下行报文用于传输客户端所需要的内容 1.3.2 网络流量路径构建&emsp;&emsp;其实就是将流量报文长度序列收集起来，生成原始序列S,即一条一维路径，也就是流量路径Xt0其中上标0表示这是原始流量路径。 给出代码提取报文长度序列，（无关原文）使用代码提取ip数据报文长度序列如下 from flowcontainer.extractor import extract pcap_path = r&quot;XX.pcap&quot; result = extract(pcap_path) for i , key in enumerate(result): print(&quot;flow&#123;&#125;&quot;.format(i)) value = result[key] print(&#39;ip lengths :&#39;,value.ip_lengths) print(&#39;\\n&#39;) 1.4 路径转换&emsp;&emsp;在大多数情况下，一维流量路径的路径特征往往不够有效，无法达到较高的分类精度。 1.4.1 路径分解变换 路径分解变换将一条路径转化为相同维数的子路径集合 一维路径Xt0首先分解为上行序列路径U0与下行序列路径D0 此时变为二维路径Xt0,d={U,D},上标d表示进行的解体变换，将另一个方向的数据包长度替换为0，即Xt0，d中上行序列U的报文长度全替换为0 变换理由：为了提高路径签名特征的效率，将流量路径转换为两条一维流量路径，这也符合客户端-服务器交互的特点。 1.4.2 累积和变换&emsp;&emsp;一般来讲，报文要传输的有效载荷通常是固定的，用于相同的网站或服务，这导致非常相似的累积和特征。 对于U0序列，累积和可如下表示：$$U’=(u_1’,u_2’,…,u_n’),u_n’=\\sum_{i=1}^ku_i$$D0序列类似 于是变换为四维，Xt0,d,c={U,D,U’,D’} 变换理由：要传输的有效载荷对于同一网站或服务往往是固定的,使用累积和变换来暴露流量路径的一些内部特征。 1.4.3 基准点变换使用函数f$$f(X_t)=(0,x_1,…,x_n)$$ 只是在路径的开头添加了一个0,这种转换使签名对路径的转换敏感，即消除了转换不变性，这对加密流量分类有利，因为不同的数据包大小代表不同的网站布局或服务内容 得到Xt0,d,c,b={f(U),f(D),f(U’),f(D’)} 变换理由：原路径签名特征具有平移不变性，但加密流分类问题不需要这一属性，因此使用基点变换对流量路径进行变换。 1.4.4 结合时间变换 通过添加单调坐标(如时间)来丰富原有的线性路径，从而保证签名的唯一性。它通常对应于添加时间参数化作为路径坐标。 将交通路径5维路径Xt0,d,c,b,t={f(U),f(D),f(U’),f(D’),t 变换理由：保证构建的流量路径具有唯一性 1.5 路径特征抽取 使用滑动窗口机制进行多尺度特征抽取 提取分层特征，设置q层，有q个滑动窗口，对于第i层的滑动窗口Wi,窗口长度与步长都是n2i-1，这将产生2q-1个子路径，自路径长度分别为n,n/2,n/4,…,n/2q-1 即对于第一层，窗口长度为n，步长为n，产生2^0=1个子路径 对于第二层，窗口与步长都是n/2, 产生2^1=1个子路径 对于第三层，窗口与步长都是n/4，产生2^2=4个子路径 …… 对于第q层，产生2^q-1个子路径故总的路径数目为$$2^0 +2^1 + ···+2^{q-1}=2^q-1$$&emsp;&emsp;然后对于这2^q-1个子路径进行计算路径签名，作为分类器输入特征 整体流程如下所示： 图中CS Sequence表示累积和系列 2、实验2.1 数据集 2.2 预处理 分为session双向流，提取数据包长度 标记 过滤：对于流太短的，只有几个包，剔除；类别样本数目太少的，剔除 2.3 调参 序列长度：40 236个类别，每个类别45个样本 2.3.1 序列选择的调整&emsp;&emsp;上文变换后的Xt0,d,c,b={f(U),f(D),f(U’),f(D’)}由四个一维序列组成，但是文中有六个序列，进行排列组合变换，找出最佳变换； 最后选择选择U0序列、D0序列、U0 CS序列和D0 CS序列的组合路径，也就是段落1提到的路径组合。 2.3.2 分类器选择选RF随机森林 2.3.3 参数选择 签名深度为3窗口深度q为4 关于序列长度增长，acc降低的分析：序列长度越长，路径包含的信息就越丰富，这就对模型提取特征的能力提出了更高的要求，而固定深度的路径签名和窗口窗口深度代表了固定的模型能力，随着序列长度增加，acc降低正常 路径签名的深度：在窗口深度为2的情况下，随着序列长度从40增加到200，精度之间的差距将逐渐明显，其中深度大的情况总是优于深度小的情况。因为路径签名深度大代表高阶psf，其中包含更多的路径细节。 窗口深度：当分层窗口深度从2增加到4时，准确率会逐渐提高。然而，大深度的分层窗口会导致PSF维度爆炸，并导致较高的计算成本。…… 2.4 在开放世界数据的效果 3、总结与思考缺陷：数据包填充技术会使得基于序列特征的方法失效。 &emsp;将深度学习应用于带路径签名的加密流分类是一个很有前景的研究方向，尤其是RNN。路径签名特征在移动加密流分类问题中的应用也很值得探索。","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文11: FlowPic_ A Generic Representation for Encrypted Traffic Classification and Applications","slug":"加密流量分类-论文11 FlowPic_ A Generic Representation for Encrypted Traffic Classification and Applications","date":"2022-11-18T13:27:14.000Z","updated":"2023-11-24T13:28:14.172Z","comments":true,"path":"2022/11/18/加密流量分类-论文11 FlowPic_ A Generic Representation for Encrypted Traffic Classification and Applications/","link":"","permalink":"https://lulu-cloud.github.io/2022/11/18/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%8711%20FlowPic_%20A%20Generic%20Representation%20for%20Encrypted%20Traffic%20Classification%20and%20Applications/","excerpt":"","text":"0、摘要&emsp;&emsp;利用了流中与时间相关和与大小相关的特性，将基本流量数据转换为直观的图片FlowPic，然后使用已知的图像分类深度学习技术cnn来识别流量类别(浏览、聊天、视频等)和正在使用的应用程序。但是不是使用负载数据形成的特征图。 1、介绍&emsp;&emsp;对于每个流，我们的方法根据数据包大小和数据包到达时间创建一个图像，我们称之为FlowPic。 不依赖于数据包有效负载内容，因此不会侵犯隐私 存储需求非常小，分类速度块，近乎实时，可以进行在线的流量分类 2、数据集介绍 数据集：ISCX VPN-nonVPN、ISCX Tor-nonTor、作者团队自己捕获的数据集（命名为TAU） 分类标签类别： VoIP Video Chat File Transfer Browsing 因此，对于五个类别，三种加密技术(非VPN、VPN、Tor)，相当于15种流量。 显然这是关于流量类型识别 2.1 数据处理&emsp;&emsp;主要是写作者自己数据的收集的一些细节。 2.2 数据增强 单向流分大小相等的块，实验中为每隔60s分为一个块 但是为了进行数据增强，就将两个块之间进行重叠，重叠时间设置为45秒，这样块与块之间的间隔为15秒 数据扩充过程是在将所有会话分割为一个训练集和一个测试集之后进行的，确保训练块和测试块之间在单个会话中没有重叠 （数据增广后的每个类别的样本数目） 2.3 敏感性分析关于数据增强是否真的有效？块长大小为多少合适？ 结论： 在个别的流量类型里，数据增强效果不明显 60s的块大小最合适 3 构建图像 3.1 构建FlowPic 提取每个单向流中的每个数据包的两个特征IP包大小、到达时间 构建一个基于流的二维直方图的图像,该图像可以被视为负载大小分布(PSD) X轴为包的到达时间，Y轴为包的大小 绝大多数包的大小都不超过1500字节(这是以太网MTU值)，将y轴限制在1到1500之间。 对于x轴，将2d直方图设置为正方形图像。为此，我们将所有到达时间值标准化为0到1500之间(即60秒映射为1500) 生成1500x1500的直方图，直方图命名为FlowPic，存储在矩阵当中，作为模型输入 3.2 FlowPic分析&emsp;&emsp;这里说了作者在对生成FlowPic的一点分析，从而说明FlowPic能反映出网络流通特征复杂，使用深度神经网络模型进行特征提取并分类是很有必要的。 分析1：在不同应用下，对视频流的分析： 不用应用下的流量类型表现不同特性，例如，Netflix传输的数据包大小几乎是固定的，而Skype、Facebook和谷歌Hangout等应用程序传输的大小分布广泛。并且，视频流不仅限于显示元素，还包括行为与VoIP相同的音频流，以及看起来像聊天传输的用于协调和控制的小数据包流。相比之下，例如在Skype上，视频流和音频流是分开的。 分析2：加密技术对流量类别的流行为的影响： 在不同的加密技术之间，有些类别的flowpic行为完全不同 分析3：Tor的加密技术下，Tor流量的包的大小分布比较离散，从图中可以看出来，与非vpn流量中的许多包大小不同。 4 卷积神经网络结构设计 输入：二维1500x1500图像 输出：2或者流量类别（2是判定是否为NonVPN） 延迟分析：TBS +TFC +TML TBS是自定义块大小(15、30或60秒) TFC是FlowPic构建时间 TML是执行分类的CNN运行时间。 &emsp; &emsp;实验中，我们发现TFC和TML都是0.1 s，与块大小相比可以忽略不计，故可以满足在线分类要求。 5、实验 5.1 处理样本不平衡问题方法：过采样、欠采样 5.1.1 多类分类情况 流量类型分类（Traffic categorization）：对于三种数据集（非VPN、VPN和Tor）合并其中相同类型的，而不考虑加密技术，动机是研究加密技术如何影响流量行为。 加密技术分类：即3分类，识别出是否为非VPN、VPN和Tor三种的某一种流量。 应用识别：使用创建的数据集，在VoIP类型与视频类型下捕获10个应用程序的三种加密方式（非VPN、VPN、Tor）的流量。 5.1.2 一对多的分类情况&emsp;&emsp;为3种加密技术构建类与所有数据集:非VPN、VPN(针对所有类，除了browse)和TOR，以及合并数据集。对于每种加密技术，每个流类别合并数据集包含相同数量的会话。 训练测试集比例是 9:1 Wang等人link)使用每个流的前784字节对ISCX VPN-非VPN数据集上的流量进行分类，并使用不同的表示方法分别对非VPN和VPN流量实现了83.0%和98.6%的最佳准确性。但Wang的实验没有包括浏览类别，因为很难将其与其他类别区分开来。从上图的混淆矩阵可以看出，，难以区分浏览和聊天是导致准确度下降的主要原因。 与其他方法的对比 还有很多的实验结果：比如未知流量识别、加密技术分类、应用分类，不一一列举。 5、总结与思考 亮点：FlowPic的图生成很好，模型分类快，能进行在线分类，不依赖于双向流信息 只考虑时间特征，可以结合空间特征，构造常规的有效载荷流量图，然后进行结合着进行分类？","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文10: Global-Aware Prototypical Network for Few-Shot Encrypted Traffic Classification","slug":"加密流量分类-论文10：Global-Aware Prototypical Network for Few-Shot Encrypted Traffic Classification","date":"2022-11-11T13:27:14.000Z","updated":"2023-11-24T13:37:26.190Z","comments":true,"path":"2022/11/11/加密流量分类-论文10：Global-Aware Prototypical Network for Few-Shot Encrypted Traffic Classification/","link":"","permalink":"https://lulu-cloud.github.io/2022/11/11/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%8710%EF%BC%9AGlobal-Aware%20Prototypical%20Network%20for%20Few-Shot%20Encrypted%20Traffic%20Classification/","excerpt":"","text":"0、摘要 现在大部分对于小样本学习的方法都是基于度量(metric learning)解决，但是这些方法只考虑到了流量的局部信息，故对最终的分类性能有一定影响 本文提出的GP-Net，考虑负载序列的两个字节之间的关系，利用字节之中的关系聚合流量输入的全局信息 少样本学习是元学习的在监督学习领域的应用，可以参考link](https://zhuanlan.zhihu.com/p/156830039)) 1、概念介绍1.1 基于metric的少样本学习的方法 过去使用一维卷积神经网络作为编码器解码器提取流量特征的方法，如RBRN)因为卷积核大小的限制，难以提取整个流量的全局信息，因此导致后续计算相似度时，不够精确。 1.4 文章核心观点引入 论文模型优势：在新流量类型样本不足的情况下学习更好的表示方法 四大模块： 流量归一化：将原始流量（输入是流量的有效载荷信息）转为图片 全局感知表示：基于自注意力的思想，并且对负载序列中字节位置信息进行建模，克服自注意力机制中的位置不可知的缺陷，这样就能聚合流量的全局信息. 所谓全局感知，其实就是引入了自注意力机制而已 嵌入生成器：卷积操作，因为前面的全局感知，使得这里的卷积不同于以前论文方法的卷积，全局而不局限 计算相似度模块：与以前方法类似。 通篇下来就是全局两个字 2、初步知识2.1 元学习概念 &emsp;&emsp;元学习概念：与传统的机器学习不同，元学习的基本单元是一个任务而不是一个训练示例。元学习的主要目标是从元训练任务中获取通用的元知识，并将其用于只需少量样本的元测试任务的快速学习。如上图：训练任务与测试任务是不相关的，用不同颜色与形状表示出来，元学习期待从完成元任务中学习到的知识能很好地迁移到待试验数据上。 2.2 问题定义&emsp;&emsp;假定两个流量数据集，都是有标签的，定义为： D^{tr}=\\{(x_1^{tr},y_1^{tr}),(x_1^{tr},y_1^{tr}),···，(x_K^{tr},y_K^{tr}),\\} 其中，x_i^{tr}∈R^d，y_i^{tr}∈\\{1,2,···，C\\} D^{test}=\\{(x_1^{test},y_1^{tr}),(x_1^{test},y_1^{test}),···，(x_K^{test},y_K^{test}),\\} 其中，x_i^{test}∈R^d，y_i^{tr}∈\\{C+1,···,C+N\\}&emsp;&emsp;意味着Dtr中有C个类别的流量，并且每一种类的流量都有一定的样本，但是，在Dtest中，样本数量非常少，并且类别都是Dtr中没有的。 &emsp;&emsp;将训练集与测试集中的任务定义为： \\tau=\\{\\tau_A^{tr},\\tau_B^{tr},\\tau_C^{tr},···\\}每一个元素代表一个任务，是二元分类，如上图，都由支持集(support set)与查询集(query set)组成，叫法这么叫而已，其实就是训练集与验证集。 &emsp;&emsp;支持集与查询集的形成遵循N-way、K-shot原则，即N个流量类型，每个类型抽k个样本。这样在测试任务中，即使测试任务的支持集很小，但是在元学习的表现下，仍然有不错的泛化性。 3 GP-Net3.1 概览 3.2 流量归一化模块&emsp;&emsp;经典三大步：分流、地址匿名、转为图片形式 分流：这里没说是单向流还是双向会话流，使用工具SplitCap 地址匿名：MAC、与IP地址匿名化 转图片：取载荷前784（28*28）字节，超则截，短则填 3.3 全局感知模块（Global-aware representation）&emsp;&emsp;对整个流量输入信息进行聚合,并且引入相对位置机制对字节的位置信息进行建模。 3.3.1 全局信息增强&emsp;&emsp;基于CNN的方法由于卷积核的大小关系，无法捕获有效载荷字节之间远程关系，堆叠CNN后，会造成参数多、存在过拟合的问题，这在少样本的情况下尤为明显。 &emsp;&emsp;而注意力机制不会，无视空间距离，考虑每一个载荷之间的相互关系。 对于一个流量图片X，将其拉直，经过运算得到特征向量A： 3.3.2 相对位置机制&emsp;&emsp;但是，上式子中，自注意机制是位置不可知的。此属性丢失了字节的位置信息，导致流量表示不全面。于是改进，加入相对位置，A表示如下： Prel是相对位置矩阵，比如第i与第j个字节的相对位置可以表示如下： P^{rel}[i,j]=q_ir_{j-i}其中qi是字节i 的查询向量，rj-i是字节i和字节j之间的相对位置嵌入。 3.3.3 全局增强的特征提取器（Global-enhanced feature extractor）&emsp;&emsp;通过多头注意力机制后，将不同子空间的特征向量进行拼接,得到MA： MA=Concat[A_1,A_2,···,A_N]W_O然后这里还和原始图片X做卷积的结果进行再次拼接，得到输出O: O=Concat[MA,Conv(X)] 自注意力机制+卷积结果拼接作为输出，考虑了载荷之间的距离信息。 3.4 嵌入生成器&emsp;&emsp;这里是四个相同的卷积块进行堆叠，每个都是3*3的卷积核大小，64通道+BN+ReLu+MaxPooling，得到嵌入向量e： e=f_{\\phi}(O)3.5 相似度核（Similarity Kernel）&emsp;&emsp;每一个查询样本xq转为嵌入向量eq后，与支持集中的每个类型ci进行比较，得出相似度进行分类。ci是每个i类型的样本嵌入向量评价 4、实验这里抛出三个问题： 小样本学习是否生效 注意力机制与相对位置信息是否必要 GP-Net调参 数据集：USTC-TFC2016 只有一个数据集,个人感觉不够 回答问题1： 表里的base是在训练查询集的结果，FewShot是在测试查询集的结果。 回答问题2： 消融实验表明： 第一行表示没有全局感知模块与相对位置机制的模型 第二行表示没有相对位置机制的模型 但是，我这里感觉在3.3.3 全局感知模块中，加了直接的卷积，没有对那里的卷积进行剔除，感觉这里不好，说不定模型的表现都是基于那个由X直接卷积得到的特征图，而非所谓基于全局感知模块的信息。 回答问题3： 调参，不看了 5、总结与思考 亮点：少样本学习、元学习 模型总体结构： 输入为有效载荷，结构为自注意力（加上相对位置信息）+卷积，而且注意力中并行着卷积，没有做这个的消融实验。","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文9: DarknetSec_ A novel self-attentive deep learning method for darknet traffic","slug":"加密流量分类-论文9：DarknetSec_ A novel self-attentive deep learning method for darknet traffic.....","date":"2022-10-15T13:27:14.000Z","updated":"2023-11-24T13:24:27.857Z","comments":true,"path":"2022/10/15/加密流量分类-论文9：DarknetSec_ A novel self-attentive deep learning method for darknet traffic...../","link":"","permalink":"https://lulu-cloud.github.io/2022/10/15/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%879%EF%BC%9ADarknetSec_%20A%20novel%20self-attentive%20deep%20learning%20method%20for%20darknet%20traffic...../","excerpt":"","text":"0、摘要&emsp;&emsp;提出了一种新的基于自注意力机制深度学习方法DarknetSec，用于暗网流量分类和应用识别；利用一维卷积神经网络(1D CNN)和双向长短期记忆网络(Bi- LSTM)从报文的有效载荷内容中捕获局部时空特征，集成自注意机制。此外，DarknetSec从有效载荷统计数据中提取侧通道特征，以增强其分类性能。 1、文章核心观点引入 关于目前基于深度学习的分类方法的缺陷：没有充分考虑从不同数据位置提取的局部特征之间的全局内在依赖关系和隐藏联系，最终导致分类性能不稳定 多头自注意模块的输出和自注意嵌入1D CNN和Bi-LSTM网络提取的局部时空特征同时输入到另一个注意模块中，自动捕获不同注意权重的局部时空特征之间的全局内在依赖关系和隐藏联系 用侧通道特征学习模块从有效载荷统计数据中提取特征表示 2、模型结构 感觉文章的创新点就是在模型结构了，三个分支网络提取不同的特征，从全局与局部的角度去提取流量特征。 2.1 模型总览 2.2 预处理层&emsp;&emsp;下面解释一下各个输入的预处理步骤，也就是预处理层（最下一层）干的事情。 预处理pcap或者是pcapng文件，将有两个方向相同的五元组的一组数据包作为流（相当于双向流：会话），在提取五元组网络流时，我们去掉包头，只保留每个包的应用层数据。 网络层和传输层的协议字段是数据包的基本组成部分，但它们主要是为了网络传输而设计的，而不是为了识别应用程序，故可以剔除 应用层以下的协议字段包含的有效信息很少，不能为细粒度流分类提供充分的区分特征。 Content features:简言之，提取流前N个packet的前M个字节，长则截断，短则填充0 xcontent表示一个样本流的内容特征，所有字节值处于0xff，映射到[0,1] Side-channel features：由统计（statistical）特征与序列特征（sequential）组成 选择流前L个包的长度序列作为序列特征 统计特征则有很多，如 流持续时间 数据包之间的时间间隔（最大值、最小值、平均值、标准差、中位数等等） 包长度统计信息（最大值、最小值、平均值、标准差、中位数等等） 接受包统计信息（最大值、最小值、平均值、标准差、中位数等等） 发送包统计信息（最大值、最小值、平均值、标准差、中位数等等） 入包出包数、字节数、每秒出（入）包比 2.4 特征提取层 Side-channel features送入MLP进行处理 Content feature复制成两份，一份送入多头注意力模块提取全局特征，一份送入局部时空特征学习模块，提取局部的时空关联。由1维卷积与Bi-LSTM组成。最后两个模块的输出进行基于注意力内容的融合（从全局角度获取网络流不同数据位置之间的内在依赖关系，有助于综合内容特征的学习），输出Oacff： 与上面MLP输出的向量进行拼接后送入分类层 4、实验 超参数分析：最终敲定N=30，M=256，L=100 消融实验：证明各模块有效 5、总结与思考&emsp;&emsp;为什么对侧通道特征提取用MLP，对内容特征提取用attention+CNN+LSTM？可不可以做排列组合，这些部件是否有改进或者说舍弃的必要，使得模型具有轻量性。","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文8: An Encrypted Traffic Classification Method Combining Graph Convolutional Network and","slug":"加密流量分类-论文8：An Encrypted Traffic Classification Method Combining Graph Convolutional Network and ...","date":"2022-10-13T13:27:14.000Z","updated":"2023-11-24T13:35:04.771Z","comments":true,"path":"2022/10/13/加密流量分类-论文8：An Encrypted Traffic Classification Method Combining Graph Convolutional Network and .../","link":"","permalink":"https://lulu-cloud.github.io/2022/10/13/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%878%EF%BC%9AAn%20Encrypted%20Traffic%20Classification%20Method%20Combining%20Graph%20Convolutional%20Network%20and%20.../","excerpt":"","text":"0、摘要&emsp;&emsp;构造了一个k -最近邻(KNN)交通图来表示交通数据的结构，从流量结构和流量数据中学习特征表示，利用两层图卷积网络(GCN)架构进行流特征提取和加密流分类。进一步使用自动编码器学习流数据本身的表示，并将其集成到gcn学习的表示中，利用了GCN和自编码器的优点，在只需要少量标记数据的情况下就能获得较高的分类性能。 1、文章核心观点引入 样本的结构可以揭示标记样本与未标记样本之间的潜在相似性，为标记样本较少的分类任务提供有价值的指导 在网络流量分析领域中，构建流量图来描述流量结构的方法有很多种，但是这些图主要包含了网络的通信模式和拓扑信息，很少包含流量相似度的信息 构造k -最近邻图来表示交通数据的结构。我们将交通流作为KNN图中的节点。对每个节点，通过相似度计算找到其前K个相似点作为其近邻，并建立边缘连接。进一步利用图卷积网络(GCN)获取交通结构信息，进行流特征提取和分类 ​ 如图，相当于两个信息分支： 关键去找论文如何定义所谓的相似度 2、模型方法2.1 预处理(Data Preprocessing) 流量分割（Traffic Split）: &emsp;&emsp;去掉pacp文件头的前24个字节，此24字节只包含pacp文件的统计信息，然后基于5元组分成流（flow）的形式，原始流量就转换成流集合F：$$F=[f_1,f_2,…,f_{n}]$$对于每一个fi,都包含q个包（packet）$$f_i=[p_1^i,p_2^i,…,p_{q}^i]$$ 流量过滤（Traffic Purification）: &emsp;&emsp;删除每个数据包p中的MAC头，因为它被两个MAC地址填充，也可使它们为零来匿名化这五个元组，并且去掉了所有重复的和空的流文件，避免了对我们分类模型学习能力的不利影响 统一长度（Length Unification）: &emsp;&emsp;对于大于900字节的流，将其裁剪为900字节，对于小于900字节的流，在流的末尾添加0x00，使其补充为900字节 数据归一化（Data Normalization）: &emsp;&emsp;将900字节的流序列转换为900维的向量。然后，我们将流向量归一化到范围[0,1] &emsp;&emsp;经过上述转化，每一个流fi都转为流字节向量FBV，原始数据转为矩阵X∈RN*d，N代表N个流，d代表一个FBV的维度 2.2 流量图形构建(Traffic Graph Construction)&emsp;&emsp;对于流数据X，找到对于每个流最相似的k个流，每个流f作为图的一个节点，找到最相似的流作为相邻的节点，在它们之间设置边。对于两个向量xi,xj,相似度计算公式如下： 其中指数的分子是表示计算两个向量的欧式距离，在计算相似矩阵S后，选取每个流的top-k相似点作为其近邻，构造无向k最近邻图。这里原文给出了一个依据KNN构造图与原始图（trace graph）的差距，通过相似度计算构建KNN流量图，使得相同应用类型的流量之间建立了更多的连接。KNN图比迹图更容易区分不同类型的流量。 2.3 分类模型&emsp;&emsp;分类模型由GCN与SAE两个部分组成 GCN：将加密的流量分类转换为节点分类任务，考虑两层GCN架构来获取流量结构信息： 第一层GCN的输入：邻接矩阵A和流量数据X。A是图的邻接矩阵，X是原始流量数据的矩阵 第一层输出Z(1) 其中：$$\\widehat{A}=A+I$$ D是度数矩阵，D^是对角阵，每个值是A^矩阵的行和，W是权重参数矩阵，外面是激活函数 第二层GCN： 损失函数为交叉熵损失。 SAE：输入为原始流量X，输出为重构的X，最后一层编码器输出为He，损失函数为均方差MSE损失，故模型总损失为两损失之和。 以上两个部分并没有联结，此处作者的创新将SAE编码器的压缩输出作为辅助信息，与GCN第一层融合，送入GCN第二层，进行分类。由此有如下的Representation Delivery Representation Delivery:由于KNN流量图是通过相似度计算构造的，因此在KNN图的同一连通分量中，节点的特征更加相似。图卷积运算可能会使它们趋向于收敛于同一值，可能会混合来自不同簇的节点的特征，使它们难以区分，导致过平滑，为了缓解，进行Representation Delivery：$$\\widetilde{Z}=(1-\\phi)Z^{(1)}+\\phi H^M_e$$ $$\\phi是一个平衡参数$$ ，故GCN输出改为：$$Z^{‘}=softmax({\\widehat{D}}^{-1/2}\\widehat{A}\\widehat{D}^{-1/2}{\\widehat{Z}}^{(1)}W^{(1)})$$这样可以增强节点的特征表示能力，防止GCN过度强调相邻节点的关联而忽略节点本身的特征，从而提高模型分类性能。 4、实验 由于是半监督模型，关于标记率的分析 分析：在所有方法中，随着准确率上升，acc都有增加的趋势，但所提方法的曲线始终高于其他方法，而我们方法的准确率始终优于基线方法。对比结果表明，在低标记率的情况下，将流量结构信息与流量数据相结合进行加密流量分类是有效的。从图中还可以看出，当标记率从10%下降到1%时，我们的方法的准确率下降速度明显慢于其他方法，说明我们的方法具有更好的鲁棒性。 超参数分析： K与平衡参数\\phi K取3,5,7比较好 \\phi取0.5比较好 似乎没有做消融实验，似乎没有较强说服力说明这个Representation Delivery是有效的…… 5、总结与思考&emsp;&emsp;Representation Delivery的创新，感觉这方面没有做相应的消融实验说不过去，毕竟创新点就是这个，不做消融实验怎么说明它一定是有效的捏。","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文7: MEMG_Mobile Encrypted Traffic Classification With Markov Chains and Graph Neural Network","slug":"加密流量分类-论文7：MEMG_ Mobile Encrypted Traffic Classification With Markov Chains and Graph Neural Network","date":"2022-10-06T13:27:14.000Z","updated":"2023-11-24T13:24:33.294Z","comments":true,"path":"2022/10/06/加密流量分类-论文7：MEMG_ Mobile Encrypted Traffic Classification With Markov Chains and Graph Neural Network/","link":"","permalink":"https://lulu-cloud.github.io/2022/10/06/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%877%EF%BC%9AMEMG_%20Mobile%20Encrypted%20Traffic%20Classification%20With%20Markov%20Chains%20and%20Graph%20Neural%20Network/","excerpt":"","text":"0、摘要&emsp;&emsp;本文提出了一种基于马尔可夫链和图神经网络(MEMG)的移动加密流量分类方法。我们利用马尔可夫链来挖掘流中隐藏的拓扑信息。然后在此基础上构建流图结构，在图的节点特征中加入流量的序列信息。我们还设计了一个基于图神经网络的分类器，从图中学习拓扑和顺序特征。分类器可以将图结构映射到嵌入空间中，并通过嵌入向量差对不同的图结构进行分类。 1、概念介绍1.1 综述部分1.1.1 基于机器学习的加密流量分类 依赖于统计特征： 包级别的统计特征，包括接收和发送包数的平均值、最小值和最大值。 TLS的时间分布、未加密的报头信息和流元数据为特征 流的统计特征和突发元数据（burst metadata）中的统计特征 依赖于流级特征（Flow-level feature）：一般方法根据流中的每个包提取流的生成概率 利用加密流量的消息类型序列，构造生成概率最大的一阶马尔可夫模型对加密流量进行分类 根据证书长度和首包长度，以提高二级马尔可夫模型下的流分类任务的性能 利用包长度马尔可夫随机场(MRF)转换矩阵作为结构特征，构建了加密流量的指纹 1.1.2 基于深度学习的加密流量分类 DeepPacket借助1-D CNN与堆栈自编码器，通过提取有效载荷对流量进行分类（包级） 有的利用注意力机制对有效载荷进行编码，最后进行分类 &emsp;&emsp;缺点：没有考虑客户端到服务器之间的交互运动等其他流信息，计算开销大 1.2 文章核心观点引入 使用一阶马尔可夫链来构造流的拓扑结构，捕获流的隐藏拓扑信息和序列信息，称为马尔可夫图。节点为马尔可夫跃迁状态，边为跃迁概率。 流中每个包的上下文序列添加到图结构中作为节点特征，通过图这种数据结构，将拓扑信息与序列信息结合在一起，将流分类问题转化为图分类问题。 GCN与MLP自动从拓扑结构信息和序列信息中提取相应特征，并将两种特征进行融合。 GCN提取拓扑特征 MLP学习序列特征 2、图构造2.1 问题定义&emsp;&emsp;对于流序列的表示，有许多中序列表示方法表示一个流，如包长序列、消息类型序列等。这里取其中一个，假定N个样本，M个应用，对于数据集中的第i个样本xi$$x_i=[p_1^i,p_2^i,…,p_{li}^i]$$li代表样本xi的长度，xi的每一个分值xij代表时间序列为j的值 这里并没有明说这个值到底是啥，是包长？负载？ 2.2 序列转换&emsp;&emsp;考虑一个五元组流的前100个数据包的长度信息，将这个100个数据包作为一个流，转为一个一阶马尔可夫链。假定MTU为1500byte，那么设定10个状态Sj,j从1-10每个状态的长度区间为150。譬如：1-150长度为状态1；…1351-1500长度为状态10。计算状态转移矩阵W，并将状态转移矩阵W按行归一化，即每一行的元素之和为1. 2.3 节点与节点状态 节点：由于根据数据包长度的大小将原始数据包长度序列转换为状态序列，所以马尔可夫图中的每个节点都包含多个数据包，其长度值属于相应的状态。 节点特征：由于每个节点可能包含多个数据包，故数据包的顺序信息可能会难以提取，故对数据包状态序列进行切片操作： 每个数据包的上下文是状态序列的前2个数据包和后2个数据包，作为该数据包状态的顺序信息 对于处于相同状态的所有包，使用RNN对那些包的上下文进行压缩，最后形成p维向量，p取128，作为节点特征 2.4 马尔科夫图的构造 &emsp;&emsp;这里给出了前面所有步骤的算法描述: 输入：流序列、状态空间、窗口大小 输出：马尔科夫图 具体描述： 1-3行的循环：将原始流序列Xi转为状态序列Bi（4），对S取模，这里S=150 5-7行的循环：通过得到的状态序列Bi构造马尔科夫图的节点集合 8：计算状态转移概率矩阵M 9-15：将M的值作为权重赋给马尔科夫图的边 16：初始化状态特征集合Fi 17-19：对窗口w内的上下文状态序列，进行切片，作为状态序列的顺序信息添加到Fi中 20-22：对F特征集合的对每一个特征，利用RNN压缩映射到128维向量，作为节点的特征向量 23：得到最终的马尔科夫图G 2.5 马尔科夫图MG的优势&emsp;&emsp;这里作者任务MG至少包含流量的两种特征： 马尔科夫链能包含流量的拓扑信息 包长度序列也能在拓扑信息中表示 3、图神经网络结构 &emsp;&emsp;原始流量转为图结构后，使用GCN与MLP进行特征抽取进行分类。 &emsp;&emsp;对于图中橙色模块的，分为特征提取与特征池化 特征提取： 提取图的全局拓扑信息：使用GCN，每次使用当前节点的邻接节点进行节点状态更新，得到F1 提取每一个节点状态的顺序信息：使用MLP，得到F2 特征池化：减少参数，获得更鲁棒的泛化，缓解过拟合$$F_{fusion}=Activate(MLP([F_1;F_2]))$$[;]表示拼接，根据对应的分数对节点进行排序，取前k个节点的特征，论文中k取5 为了考虑到模型结构的全局与局部特征，对每个橙色结构块的输出进行拼接考虑，作为最终分类器的输入（图中的Readout模块） 分类器模块：MLP结构，采用交叉熵损失。 4、实验 与机器学习方法与其他的马尔科夫族方法做了对比 似乎没有做消融实验，不能证明模型每个结构的必要性 实验证明在精度性能的优越性下，MEMG保证的小的内存开销与快速收敛 似乎也没有跟目前的主流的深度学习方法做对比 5、总结与思考&emsp;&emsp;马尔科夫图+GNN可以凑一起，并且理由解释得很好，什么拓扑信息、顺序序列信息怎么怎么提取。","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文6:Learning to Classify A Flow-Based Relation Network for Encrypted Traffic Classification","slug":"加密流量分类-论文6：Learning to Classify A Flow-Based Relation Network for Encrypted Traffic Classification","date":"2022-10-02T13:27:14.000Z","updated":"2023-11-24T13:19:05.448Z","comments":true,"path":"2022/10/02/加密流量分类-论文6：Learning to Classify A Flow-Based Relation Network for Encrypted Traffic Classification/","link":"","permalink":"https://lulu-cloud.github.io/2022/10/02/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%876%EF%BC%9ALearning%20to%20Classify%20A%20Flow-Based%20Relation%20Network%20for%20Encrypted%20Traffic%20Classification/","excerpt":"","text":"0、摘要加密流量分类的挑战性问题: 网络数据的不平衡性 模型对真实数据的泛化能力 模型对数据大小的过度依赖。 1、概念介绍1.1 流量加密背景&emsp;&emsp;加密技术虽然保护了互联网用户的自由、隐私和匿名性，但也用户避开了防火墙的检测，绕过了监控系统。 &emsp;&emsp;由此导致的问题： 攻击者通过加密恶意软件流量来匿名入侵和攻击系统。 犯罪分子使用隐私增强工具(例如Tor)穿透黑暗网络，在那里他们可以购买毒品、武器和伪造的文件(如护照、驾照、提供合同杀手的媒体)来吸引客户. 1.2 流量分类技术的发展（综述部分） 基于端口号 基于有效载荷,也就是DPI方法。1,2只能针对非加密流量 基于流统计特征的方法，分类性能取决于人类的特征工程 基于深度学习的方法： 优点：端到端、具有相当高的学习能力 缺点： 不能解决数据中类的不均衡问题 训练好的模型不能很好适用于真是流量环境，泛化能力差 过分依赖数据集的大小与数据分布的好坏 1.4 文章核心观点引入&emsp;&emsp;对于一个新奇概念的样本，人类可以轻易根据对事物共同变化模式的了解，想象该样本在其他环境的样子。如果机器从辨别出在不同环境（原文这里称作幻觉（hallucination））下的同一样本，那么通过一个致幻器（hallucinator）产生额外的训练样本，理论上就可以在少量数据样本的情况下学习到不错的泛化能力。 &emsp;&emsp;将从有限的数据中提取先验知识并将其转移到不可见的任务或者环境中，元学习是一种解决方案。（不是半监督模型预训练那一套） 关于元学习（学习如何去学习）： 元级（meta-level）学习器：在不同的训练任务以及对应的训练数据上进行学习，以此作为网络结构的先验知识，下图的F。 基础级（base-level）学习器：为特定任务设计的学习器。由F针对测试任务学习出来的f &emsp;&emsp;在元学习阶段，进行多任务学习，称作Training Task，每个任务都有自己的训练集与测试集，也称为支持集（support set）与查询集（query set），学习到一个F。在基础级学习阶段，进行特定任务的学习，称作Testing Task，将Testing task的给入F，得出学习到的f，然后在f上进行传统的训练测试，用f的表现去反映F的泛化能力好坏。 分类粒度：基于流的分类（这里是原文：流序列作为唯一的原始流量信息。原始流可以表示为几个具有相同流长度和不同类型的序列(例如，消息类型序列和包长度序列)。一般我们把一种序列看作流序列，其他的序列也可以用同样的方法。）类似于FS-Net的输入，所以此模型输入应该是类似与FS-Net的输入，比如是取原始流的包长序列或者消息类型序列作为流序列信息，输入到模型 分类目标：应用分类 本文的分为训练集、支持集、测试集（支持集和测试集共享相同的标签空间，但训练集有自己的标签空间），原则上只需要支持集与测试集就可以了，但是支持集中缺乏标记样本，使用训练集上的元学习将提取的知识转移到支持集 训练集合切割成为支持集与查询集？？？ 2、模型结构2.1总览 &emsp;&emsp;本文提出基于流的关系网络分类模型（FlowBased Ralation Network，RBRN），从原始流序列学习特征，是端到端的分类模型。 致幻器：产生额外样本（相当于一个数据增强器？） 编码器：生成样本特征 解码器：恢复输入序列，也进行特征的学习。编码器解码器都是多层CNN结构 基于元学习的分类器：分类粒度，对应用程序的分类 2.2 集合分割 从训练集中选择跟支持集一样规模的作为集合S：假定支持集n个类，每个类m个样本，从训练集也随机选n个类，每个类选m个样本，剩余的作为查询集。 2.3 致幻器（Hallucinator） Hallucinator的测试流程： 从原始数据集S中对每一个类别进行采样。 将采用后的样本添加噪声z，送入流产生器Flow（图中红色部分），输出数据增强后的样本Sg 将Sg与原始数据S合并为Saug，并作为最终的训练数据，送入h（分类算法）中进行预测。 Hallucinator的训练流程： 采用元学习的方法来训练Hallucinator与后面的分类器h，要求h对于Saug的元素可微，以便进行梯度下降，Hallucinator与后面的网络结构是同步进行训练学习的。 这里目前还没有提及到关于元学习的多任务。 2.4 编码器（Encoder）与解码器（Decoder） 编码器输入：Saug，输出压缩特征。 体系结构：VGG16网络中的前13个卷积层，舍弃全连接层，理由：以便在最深的编码器输出处保留更高分辨率的特征映射（论文讲的理由）。 这里说卷积后的特征应该存储起来，但是由于内存限制，采用存储池化层的索引 即池化窗口的最大特征值的位置被存储起来 解码器输入：编码器输出的压缩特征，输出提供给基于元学习的分类器。 从相应解码器存储的最大池化层索引来进行采样 卷积层的滤波器仍旧是可训练的 2.5 基于元学习的分类器 假设样本xj来自查询集Q，样本xi来自增强后的数据集**Saug 通过编码器-解码器输出后为zj=fencoder-decoder(xj)为zi=fencoder-decoder(xi) 定义样本通过特征抽取模块的输出为mj=fmeta-network(zj)和mi=fmeta-network(zi)** 将mj与mi联结起来，通过关系模型 Jrelation计算两个样本之间的相关度 采用与最后查询集的实际类形成的独热向量之间的均方差作为损失，构建损失函数 4、实验 在小样本学习有不错的表现：在Full ISCX vpn -非vpn流量数据集中，为每个应用程序选择1000条流量记录作为训练集，其他的作为测试集，仍旧有不错的效果。 在数据不均衡样本集有不错的表现 在复杂环境时能保证不错的泛化能力：在均衡样本集中训练，在非均衡上测试，对比其他模型，表现出不错的效果 不太理解在均匀数据集上训练的模型在非均衡数据集上进行测试，如何能体现出泛化能力。 5、总结与思考 从李宏毅老师学习到元学习的有关概念，感觉这里元学习作者没有太突出出来，我的理解是训练集很大，每次选一个子集合作为支持集，就相当于一个元学习任务了，这样多次选训练集子集，相当于对元分类器进行多任务训练。……不知道理解对不对。。。 网络模型挺深，模块多，实验部分做了消融实验以此证明每部分都是有必要的。 不太理解在编码器解码器中对于池化索引的记录的点，以及解码器的存在的意义，只需编码部分可不可以?","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文5:MATEC_A_lightweight_neural_network_for_online_encrypted_traffic","slug":"加密流量分类-论文5：MATEC_A_lightweight_neural_network_for_online_encrypted_traffic","date":"2022-09-27T13:27:14.000Z","updated":"2023-11-24T13:13:32.888Z","comments":true,"path":"2022/09/27/加密流量分类-论文5：MATEC_A_lightweight_neural_network_for_online_encrypted_traffic/","link":"","permalink":"https://lulu-cloud.github.io/2022/09/27/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%875%EF%BC%9AMATEC_A_lightweight_neural_network_for_online_encrypted_traffic/","excerpt":"","text":"0、摘要 &emsp;&emsp;现有的深度学习方法为了获得高精度的分类结果而牺牲了效率，已经不适合大量加密流量的场景，本文提出了一种实现为MATEC的轻量级在线方法，遵循“精简模块重用最大化”的设计原则（Maximizing the reuse of thin modules）。 1、问题引入 &emsp;&emsp;老规矩，先说其他方案的缺点： 统计特征+基于机器学习的方法：要获得流量的统计特征，需要观察流的全部或者大部分，内存开销大，只能适用于离线分类。 基于深度学习的方法：为了追求精度，目前提出的神经网络运行时空开销大，效率不够高。 所以本文提出了MATEC的轻量级的神经网络，可以用于在线流分类，输入是流中的随机位置的三个数据包；基本结构：多头注意力机制+一维CNN；期望能提取全局（流级）与局部（包级）的特征，全局特征来自流中信息包之间的交互，而局部特征则包含在一些原始信息包的字节中。 此外，这篇文章的方法通过迁移学习，也对零日应用的流量探测有一定效用。 2、流分类的相关工作（综述部分）2.1 加密流量分类 &emsp;分类目标： 基于协议(如HTTP、SSL、SMTP、DNS或QUIC) 基于流量类型(如视频、聊天或浏览) 基于应用程序(如Amazon、Apple、Microsoft或谷歌) 基于网站 &emsp; 分类方法是否在线，可以分为： 在线流量分类（本文提出的是在线方法） 离线流量分类 常见流量分类方法，包级（Packet-Based）与流级(Flow-Based),然后介绍了什么是包、什么是流。 流是指所有具有五元组(即传输层协议、源IP、源端口、目的IP、目的端口)相同值的报文，源和目的可以交换。有的论文觉得不能交换，能交换的称作为会话（session） 基于包级（报文级）的分类方法：直接将数据包的字节作为输入，包括报头信息与有效载荷。基于包的方法只关注少数包的详细信息，缺乏对全局特征的关注。（Deep Packet） 基于流级的分类方法：流级特征包括了全局信息。 基于机器学习的方法：需要手动设计并且提取统计特征，大部分必须观察整个流或流中的大部分数据包才能获得这些特征，更适合于离线分类。 基于深度学习的方法：有使用Bi-GRU对流的字节序列或者流中的包长序列进行特征提取用于分类（FS-Net），有将流中连续的几个报文作为1D-CNN的输入进行分类，有同时使用CNN与RNN分别提取流量的空间特征与时间特征。但是RNN中单元之间的依赖关系使得等待上一个单元的输出非常耗时，本文提出的多头注意力（Multi-head attention）方法就没有这种劣势。 这篇文章的综述特别详尽 3、模型结构3.1嵌入层（Embedding） 输入：数据流中的随机位置的连续三个包，既利用了包的统计特征（如包的长度、相对位置）也利用了包的字节特征，对于一个包xi，它的浅层特征向量如下：$$x_i={x_{i_1},x_{i_2},..,x_{i_M}}$$每一个分量代表包的一个特征，例如第一个分量可以代表包的相对位置、第二个分量可以代表包长、第三个分量可以代表删除以太网头后的第一个784字节…… 嵌入：对于xi中的每一个分量，若其是向量特征，做如下变换，$$e_{i_j}=U_jx_{i_j}$$这样将改向量映射到j维向量，若其是标量特征，则做如下变换$$e_{i_j}=u_jx_{i_j}$$这样，不论是向量特征还是标量特征都会被映射到相同的表示，转换后的向量ei如下：$$e_i=W_{map}[e_{i_1},e_{i_2},..,e_{i_n}]$$ $$W_{map}∈R^{d*m}$$ 因此，一个包就转为一个d维向量，一个流转为N*d维矩阵，这里研究N取3,即随机选取流中3个连续数据包的那个3。嵌入向量的位置编码采用绝对编码方式。 3.2 注意力编码层 &emsp;&emsp;这里期望注意力编码层能够捕获数据的高维特征，并且捕获到数据包之间的交互关系。这里的注意力层跟Transformer块很像，给出图 使用注意力机制的好处： 并行学习，运算快，参数量少 将数据映射到多个高维度，更能发掘潜在信息 3.3 全连接层 将编码后的特征作为输入，输出为预测标签的概率分布 结构可变，针对不同任务灵活改变全连接层的输出神经元个数，输入的个数是固定的，利用迁移学习优化对新交通数据的微调训练，可以加快模型的收敛速度。 4、总结与思考 模型结构声称是第一个将注意力机制引入流量分类任务。 通过调整网络结构后端的FC层，可以在新任务上通过少量标签样本进行微调，迅速收敛达到不错的效果。","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-实践1: 1D-CNN模型训练与测试","slug":"加密流量分类torch实践1：1D-CNN模型训练与测试","date":"2022-09-20T13:27:14.000Z","updated":"2023-11-24T13:10:48.431Z","comments":true,"path":"2022/09/20/加密流量分类torch实践1：1D-CNN模型训练与测试/","link":"","permalink":"https://lulu-cloud.github.io/2022/09/20/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BBtorch%E5%AE%9E%E8%B7%B51%EF%BC%9A1D-CNN%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95/","excerpt":"","text":"模型：model.py论文参数： import torch.nn as nn import torch import torch.nn.functional as F class OneCNNC(nn.Module): def __init__(self,label_num): super(OneCNNC,self).__init__() self.layer_1 = nn.Sequential( # 输入784*1 nn.Conv2d(1,32,(1,25),1,padding=&#39;same&#39;), nn.ReLU(), # 输出262*32 nn.MaxPool2d((1, 3), 3, padding=(0,1)), ) self.layer_2 = nn.Sequential( # 输入262*32 nn.Conv2d(32,64,(1,25),1,padding=&#39;same&#39;), nn.ReLU(), # 输入262*64 nn.MaxPool2d((1, 3), 3, padding=(0,1)) ) self.fc1=nn.Sequential( # 输入88*64 nn.Flatten(), nn.Linear(88*64,1024), # 自主加了两个dropout层 nn.Dropout(p=0.5), nn.Linear(1024,label_num), nn.Dropout(p=0.3) ) def forward(self,x): # print(&quot;x.shape:&quot;,x.shape) x=self.layer_1(x) # print(&quot;x.shape:&quot;,x.shape) x=self.layer_2(x) # print(&quot;x.shape:&quot;,x.shape) x=self.fc1(x) # print(&quot;x.shape:&quot;,x.shape) return x # x=torch.tensor([[1, 1, 0, 1, 2, 3], # [1, 1, 4, 5, 6, 7], # [1, 10, 8, 9, 10, 11]],dtype=torch.float32) # x=x.reshape(1,3,-1) # out_tensor=F.max_pool2d(x,(3,1),stride=3,padding=0) # print(out_tensor) 数据部分：数据读取：data.py import os from torch.utils.data import Dataset import gzip import numpy as np class DealDataset(Dataset): &quot;&quot;&quot; 读取数据、初始化数据 &quot;&quot;&quot; def __init__(self, folder, data_name, label_name, transform=None): (train_set, train_labels) = load_data(folder, data_name,label_name) self.train_set = train_set self.train_labels = train_labels self.transform = transform def __getitem__(self, index): img, target = self.train_set[index], int(self.train_labels[index]) # 这里要copy一下不然会报错 img=img.copy() # 28*28 -&gt; 764 img=img.reshape(1,1,-1) # target=target.copy() if self.transform is not None: img = self.transform(img) return img, target def __len__(self): return len(self.train_set) def load_data(data_folder, data_name, label_name): with gzip.open(os.path.join(data_folder, label_name), &#39;rb&#39;) as lbpath: y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8) with gzip.open(os.path.join(data_folder, data_name), &#39;rb&#39;) as imgpath: x_train = np.frombuffer( imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28) return (x_train, y_train) 模型训练主模块： from random import shuffle import time import sys import torch.nn as nn import numpy as np import os import torchvision from model import OneCNN,CNNImage,OneCNNC from torchvision import datasets,transforms import gzip import torch from data import DealDataset def main(): # Device configuration device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) # 设置超参数 batch_size = 50 lr = 1.0e-4 num_epochs = 40 label_num = 12 # 导入数据 folder_path_list=[ r&quot;2.encrypted_traffic_classification/3.PerprocessResults/12class/FlowAllLayers&quot;, r&quot;2.encrypted_traffic_classification/3.PerprocessResults/12class/FlowL7&quot;, r&quot;2.encrypted_traffic_classification/3.PerprocessResults/12class/SessionAllLayers&quot;, r&quot;2.encrypted_traffic_classification/3.PerprocessResults/12class/SessionL7&quot; ] # 选择哪个数据集 task_index = 0 folder_path = folder_path_list[task_index] train_data_path = &quot;train-images-idx3-ubyte.gz&quot; train_label_path = &quot;train-labels-idx1-ubyte.gz&quot; test_data_path = &quot;t10k-images-idx3-ubyte.gz&quot; test_label_path = &quot;t10k-labels-idx1-ubyte.gz&quot; trainDataset = DealDataset(folder_path,train_data_path,train_label_path) testDataset = DealDataset(folder_path,test_data_path,test_label_path train_loader = torch.utils.data.DataLoader( dataset=trainDataset, batch_size=batch_size, shuffle=True ) test_loader = torch.utils.data.DataLoader( dataset=testDataset, batch_size=batch_size, shuffle=False ) # 定义模型 model = OneCNNC(label_num) model = model.to(device) # model = CNNImage() # Loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr) # Train the model total_step = len(train_loader) for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # images=images.reshape(-1,1,28,28) images = images.to(device) labels = labels.to(device) # Forward pass outputs = model(images.to(torch.float32)) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 100 == 0: print (&#39;Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;&#39; .format(epoch+1, num_epochs, i+1, total_step, loss.item())) # Test the model model.eval() with torch.no_grad(): correct = 0 total = 0 test_length = len(testDataset) for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images.to(torch.float32)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(&#39;Test Accuracy of the model on the &#123;&#125; test images: &#123;&#125; %&#39;.format(test_length,100 * correct / total)) # Save the model checkpoint torch.save(model.state_dict(), &#39;model.ckpt&#39;) if __name__ == &#39;__main__&#39;: main() 运行结果： 项目地址：https://github.com/lulu-cloud/Pytorch-Encrypted-Traffic-Classification-with-1D_CNN","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"实验","slug":"实验","permalink":"https://lulu-cloud.github.io/tags/%E5%AE%9E%E9%AA%8C/"}]},{"title":"加密流量分类-论文4: Endtoend Encrypted Traffic Classification with One-dimensional Convolution Neural Networks","slug":"加密流量分类-论文4：Endtoend Encrypted Traffic Classification with One-dimensional Convolution Neural Networks","date":"2022-09-18T13:27:14.000Z","updated":"2023-11-24T13:06:22.652Z","comments":true,"path":"2022/09/18/加密流量分类-论文4：Endtoend Encrypted Traffic Classification with One-dimensional Convolution Neural Networks/","link":"","permalink":"https://lulu-cloud.github.io/2022/09/18/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%874%EF%BC%9AEndtoend%20Encrypted%20Traffic%20Classification%20with%20One-dimensional%20Convolution%20Neural%20Networks/","excerpt":"","text":"0、摘要&emsp;&emsp;此篇方法是第一个将端到端的方法应用到加密流量分类领域，使用数据集ISCX-VPN-NonVPN-2016数据集进行研究。 1、概念介绍1.1 流量加密技术&emsp;&emsp;依据ISO/OSI层的不同，加密技术可以分为 应用层加密：应用程序在应用层实现自己的协议以实现数据的安全传输(如BitTorrent或Skype)，在一些论文中也称为常规加密。 表示层加密 网络层加密：如IPSec加密协议 1.2 常见加密协议 IPSec协议：网络层加密协议。分为传输与隧道两种模式 传输模式：在IP报头和高层协议报头中插入一个IPSec报头，该模式不会改变IP报头中的目的地址，源IP地址也保持明文状态。 隧道模式：报文的源IP地址以及数据被封装成一个新的IP报文，并在内部和外部报头之间插入一个IPSec报头，原来的IP地址作为需要进行安全业务处理的一部分来提供安全保护，并且该模式下，可以对整个IP报文进行加密操作，常用来实现虚拟专用网VPN。 SSL/TLS协议：传输层协议。对于TLS，简单地说，是在TCP层之上再封装了SSL层。安全套接层协议 SSL 提供应用层和传输层之间的数据安全性机制，在客户端和 服务器之间建立安全通道，对数据进行加密和隐藏，确保数据在传输过程中不被改变。 SSL 协议在应用层协议通信之前就已经完成加密算法和密钥的协商，在此之后所传送的 数据都会被加密，从而保证通信的私密性。 HTTPS协议：应用层加密协议。HTTPS中，通信协议使用安全传输层TLS或者SSL进行加密。 QUIC协议：应用层加密协议。全称（Quick UDP Internet Connection），是谷歌制定的一种基于UDP的低时延的互联网传输层协议。QUIC融合了包括TCP、TLS、HTTP/2.0等协议的特性，但是基于UDP传输，主要目标就是减少连接延迟，避免HTTP/2.0的线头阻塞（Head-of-Line Blocking）问题。 1.3 端到端的流量分类方法与传统方法的对比 传统的分类方法流程：从原始流量中，经过人类专家精心设计的特征工程模块进行特征提取，形成基于流级（flow）或者包级（packet）的特征，最后送入分类器进行分类（一般是传统的基于机器学习的分类模型，如LR、SVM、C4.5等分类器） 端到端的分类方法流程：从原始流量直接进入模型，映射到它对应的label。 1.4 网络流量划分粒度&emsp;&emsp;不同的拆分粒度会导致不同的流量单位。主流的研究分为包级（packet）、流级（flow）、会话级（session） 原始流量（raw traffic）可以定义为|P|个包的集合$$P={p^1,…p^{|P|}}$$其中，$$p^i=(x^i,b^i,t^i),i=1,2,…,|P|$$ 第一个分量xi 表示五元组(源IP、源端口、目的IP、目的端口和传输级协议) 第二个分量bi表示包的长度，单位是字节（byte） 第三个分量ti表示当前包开始传输的时间 流级：一组原始流量P可以划分为多个子集，子集中的所有数据包按照时间排列，每一个子集就称为一个流（flow）$$f=(x,b,d,t)={p^1=(x^1,b^1,t^1),p^2=(x^2,b^2,t^2),…,p^n=(x^n,b^n,t^n)}, t^1&lt;…&lt;t^n$$ x表示流中所有包的五元组（都是相同的） b表示流中所有包的字节总和 d表示流持续时间，可以表示为d=tn-t1 t则是流中第一个数据包开始发送的时间 会话级：一个会话可以定义为双向流，即流的五元组中的源IP/源port与目的IP/目的port可以互换(session) 2、ISCX-VPN-NonVPN-2016数据集 ISCX-VPN-NonVPN-2016包括7种常规加密流量和7种协议封装流量 数据集结构如下： ISCX数据集的流特征有14类标签，但原始流量没有标签，因此我们根据数据集中pcap文件的论文描述对其进行标签。而一些文件，如Facebook_video.pcap可以被标记为“浏览器”或“流媒体”，所有与“浏览器”和“vpn -浏览器”相关的文件都有这个问题。决定不给这些文件贴上标签。 最后，标记的ISCX数据集有12类，包括6类常规加密流量和6类协议封装流量。 3、模型结构3.1总览 3.2 数据预处理 &emsp;&emsp;使用论文团队开发的工具USTC-TL2016进行数据预处理 流量分割：这个步骤将一个连续的原始流量分割为多个离散的流量单元。输入数据格式为pcap。如果表示类型为Flow + All或Session + All，则输出数据格式为pcap。当表示类型为Flow + L7或Session + L7时，输出数据格式为bin。即.pacp 转化为.pacp或者.bin。 ALL表示使用了所有层的包层选择 L7表示仅仅使用了OSI模型的第七层（应用层）的包层选择 流量清理：对数据链路层和IP层的MAC地址和IP地址分别进行随机化。 图像生成（非必要）：将所有文件修剪为统一的长度。如果文件大小大于784字节，则裁剪为784字节。如果文件大小小于784字节，则在最后添加0x00以补充到784字节。然后，具有相同大小的结果文件被转换为灰色图像。原始文件的每个字节代表一个像素，例如0x00是黑色的，0xff是白色的。这种转换是可选的，可以简单地直接将文件转换为IDX文件。 IDX转换：此步骤将图像转换为IDX格式文件，IDX文件包含一组图像的所有像素和统计信息。IDX格式是机器学习领域中常用的文件格式。 3.3 训练阶段&emsp;&emsp;采用小批量随机梯度下降(SGD)。采用10倍交叉验证技术，保证了CNN模型的泛化能力。 3.4 测试阶段&emsp;&emsp;利用训练好的CNN模型进行测试 3.5 1D-CNN模型 输入：取流或者会话的前n个字节作为模型输入，n取784（28*28） 使用1D-CNN的理由：CNN主要应用于计算机视觉领域，如图像分类。CNN适用于以下类型的数据: 多数组形式的数据; 具有强局部相关性的数据; 特征可以出现在任何地方的数据; 对象不受平移和扭曲影响的数据。 具体来说，1D-CNN适合于诸如顺序数据或语言之类的数据。2D-CNN适用于图像或音频声谱图等数据。3D-CNN适用于视频或体积图像等数据。网络流量本质上是顺序数据。它是一种按层次结构组织的一维字节流。字节、分组、会话和整个流量的结构与自然语言处理领域中的字符、单词、句子和整篇文章的结构非常相似。近年来，CNN在NLP中的成功应用均采用1D-CNN，如情感分析、文本分类。本文在这些研究的启发下，使用一维cnn执行加密流量分类任务，并将其性能与二维cnn进行比较。 4、实验 模型参数： 超参数设置： epoch：40 learn_rating: 1.0e-4 batch_size: 50 实验结果： 论文给出的源码是基于tensorflow实现的，这里用torch写了一个CNN做了一下exp4，也就是上图的红色箭头的结果。如下,基本差不多 class OneCNNC(nn.Module): def __init__(self,label_num): super(OneCNNC,self).__init__() self.layer_1 = nn.Sequential( # 输入784*1 nn.Conv2d(1,32,(1,25),1,padding=&#39;same&#39;), nn.ReLU(), # 输出262*32 nn.MaxPool2d((1, 3), 3, padding=(0,1)), ) self.layer_2 = nn.Sequential( # 输入262*32 nn.Conv2d(32,64,(1,25),1,padding=&#39;same&#39;), nn.ReLU(), # 输入262*64 nn.MaxPool2d((1, 3), 3, padding=(0,1)) ) self.fc1=nn.Sequential( # 输入88*64 nn.Flatten(), nn.Linear(88*64,1024), # 这里自己加了两个dropout层 nn.Dropout(p=0.5), nn.Linear(1024,label_num), nn.Dropout(p=0.3) ) def forward(self,x): # print(&quot;x.shape:&quot;,x.shape) x=self.layer_1(x) # print(&quot;x.shape:&quot;,x.shape) x=self.layer_2(x) # print(&quot;x.shape:&quot;,x.shape) x=self.fc1(x) # print(&quot;x.shape:&quot;,x.shape) return x 对比分析： 与2D-CNN的对比 与C4.5分类器的对比5、总结与思考 学习一下流量数据的预处理套路 第一个端到端的基于神经网络的分类模型？1D-CNN结果简单，分类效果好，基于深度学习的方法在流量分类中有巨大潜力。","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文3: FS-Net_ A Flow Sequence Network For Encrypted Traffic Classification","slug":"加密流量分类-论文3：FS-Net_ A Flow Sequence Network For Encrypted Traffic Classification","date":"2022-09-04T13:27:14.000Z","updated":"2023-11-24T13:04:38.318Z","comments":true,"path":"2022/09/04/加密流量分类-论文3：FS-Net_ A Flow Sequence Network For Encrypted Traffic Classification/","link":"","permalink":"https://lulu-cloud.github.io/2022/09/04/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%873%EF%BC%9AFS-Net_%20A%20Flow%20Sequence%20Network%20For%20Encrypted%20Traffic%20Classification/","excerpt":"","text":"0、摘要 &emsp;&emsp;FS-Net是一个端到端的分类模型，它从原始流中学习代表性特征，然后在一个统一的框架中对它们进行分类。采用多层编码器-解码器结构，可以深入挖掘流的潜在序列特征，并引入重构机制，提高特征的有效性。 1、问题引入 &emsp;&emsp;传统的基于统计特征加上机器学习的流量分类，太依赖与专业经验，即人类的特征工程，特征工程的好坏直接影响分类性能。以往的基于DL的流量分类方法如Deep Packet:，只使用了网络流量的有效载荷进行分类，没有考虑到流量中的其他信息。因此提出基于DL的端到端的分类模型，尝试设计一种新的适合流序列特征的神经网络结构，可以直接从原始输入中学习特征，学习到的特征以真实标签为指导，从而提高性能。因此，它可以节省设计和验证功能的人力。 2、问题定义 FS-Net是基于网络流量的应用分类，即应用识别。 一个原始流量可以表示为不同的类型序列，如消息类型序列或者包长度序列，本文将一个原始流量看作包长度序列。具体的，Xp表示第p个样本的序列表示：$$X_p=[L_1^p,L_2^p,…,L_n^p)]$$其中n是Xp的长度，L~i~^p^是时间步长i的数据包值。 3、模型结构3.1总览类似于AE半监督的思想，模型由五大块组成 嵌入层 编码层 解码层 重构层 分类器 3.2 嵌入层&emsp;&emsp; 任务：将L~1~到L~n~的序列信息转化为e~1~到e~n~的向量表示。如果有K个数据，且嵌入向量的维度为d，那么K个数据经过嵌入层将转化为一个矩阵E^K*d^,矩阵E是可以在模型训练过程中训练出来的，矩阵的每一个行向量都对应着一个数据样本的嵌入向量表示。 使用嵌入向量的优点： 一些非数值(如消息类型)可以很容易地表示为数值进行计算。 向量表示丰富了一个序列中每个元素保存的信息。嵌入向量的每个维度都是影响流生成的潜在特征。同一元素在不同的序列中可能有不同的含义和方面。 模型可以学习每个元素的嵌入向量的面向任务的较优秀的向量表示，从而提高分类性能。3.3 编码层 输入为嵌入向量，输出压缩后的特征 编码采用的是堆叠的Bi-GRU神经网络模型。低层的编码器学习到局部特征，高层的编码器学习到相对全局的特征，最后将所有层的最终前向与后向的隐藏状态串联Z~e~作为编码器压缩后的特征。此时，Z~e~就包含了整个编码流程序列的双向上下文信息，将会作为分类器的输入的一部分。（既有局部的，又有全局的）3.4 解码层 解码器的结构如同编码器一样，为折叠的Bi-GRU网络结构。 输入为Z~e~，输出由两个部分组成 1.第一部分类似于编码器的输出，为解码器所有层的前向状态与后向状态的拼接，称之为，Z~d~这部分输出将会作用与最终的分类器输出的一部分。 2.第二部分则是最后一层解码器的自身输出，这部分将会送入重构层，进行重构，重构目标是还原起初的模型输入。 3.5 分类器 分类器之前，设置了Dense层对分类器的输入（即Z~e~与Z~d~向量的拼接）进行压缩，得到新的特征向量z.然而，z的维度还是太高，使用两层带Selu的激活函数的MLP对z进行降维得到Z~c~,降维过后能有效避免过拟合问题。公式中的W1，b1,b2都是可以学习的参数。 输入为Z~c~，经过softmax分类器，得到预测标签A^-^，与真实标签A之间构造一个交叉熵损失L~C~ 在重构器后面，解码器中的Bi-GRU经过重构，输出的L~i^~与原始的输入特征L~i~之间可以构造另外一个交叉熵损失L~R~ 因此，最终的损失函数$$L=L_C+αL_R$$α是超参数。 4、实验 实验设置：以报文长度序列作为FS-Net的输出，嵌入向量维度d设置为128，GRU的隐藏状态维度也是128，α设置为1，dropout设置为0.3，Adam优化器的lr设置为0.0005 与其他模型结果实验对比的结论：加密流分类任务中，报文长度比消息类型更具有代表性。主要原因可能是[11]发现的不同应用程序的消息类型序列高度重叠。有更多的信息蕴含在包长度集合中而不是消息类型的集合中。 对FS-Net的一些分析： 摒弃解码器层、重构层和重构损失，即只将基于编码器的特征向量Z~e~传递到密集层进行分类。该变体称为FS-ND.此时FS-Net与其变体FS-ND的默认输入仍旧为包长度序列（The packet length sequence）。 个人感觉这种变体特别像BERT，BERT就是只使用了Transformer的编码器结构，经历预训练后，在诸多下游任务中均获得了不错的效果。当然，BERT是有MLM与NSP的预训练任务的，而此处的FS-ND貌似并没有提及，只是单纯砍掉了解码器与重构器那一部分。 因为传统的消息类型马尔可夫方法(FoSM、SOCRT、SOB)以消息类型序列（The message type sequences）作为输入。为了便于比较，FS-Net和FS-ND也结合消息类型序列进行测试，对应的方法记为FS-Net- s和FS-ND- s。 采用多属性序列(消息类型序列和报文长度序列)来提高性能。即同时关注包长度序列（The packet length sequence）与消息类型序列（The message type sequences），这两种不同的模型被称为FS-Net-SL和FS-ND-SL。 结果分析： 重构机制（即包含解码层、重构层）有用，提高分类性能。与不同序列比较，FS-Net的FTF性能始终优于FS-ND，提高了0.01左右。利用重构机制，引导从编码器学习到的特征存储更丰富的信息。 重构机制有用，但是对比FS-ND提示不大，并且加了那么多结构，有点不太划算。变体模型FS-ND也优于现有的模型，而且FS-Net和FS-ND之间的性能差距不大。然而，FS-ND模型比FS-Net需要更少的层，可以更快地训练。 报文长度序列的信息比消息类型序列的信息更丰富。消息类型序列的信息几乎被合并到包长度序列中。从FS-Net到FS-Net- sl的改进不显著(如FTF为0.0005)。FSND和FS-ND-SL之间也存在类似的现象。 调参分析： GRU的隐藏状态维度：太大，模型冗余，过拟合的同时容易从噪声中学习无用信息；太小，不足以提取数据的隐藏特征。研究中设置为128。 超参数α：建议α值设为[0.125,2]。5、总结与思考 模型结构，类似与NLP中的Seq2Seq结构，可否在中间的编码器与解码器之间照葫芦画瓢加上Attention机制来进一步优化捏？ 去除解码器与重构器，模型复杂度减少，并且实验证明在数据集上的表现FS-ND也跟FS-Net差之无几，能否在FS-ND上做出改进，使之效率与复杂度要比现在的模型好。","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文2: Deep Packet_ A Novel Approach For Encrypted Traffic Classification Using Deep Learning","slug":"加密流量分类-论文2：Deep Packet_ A Novel Approach For Encrypted Traffic Classification Using Deep Learning","date":"2022-09-02T13:27:14.000Z","updated":"2023-11-24T13:04:24.900Z","comments":true,"path":"2022/09/02/加密流量分类-论文2：Deep Packet_ A Novel Approach For Encrypted Traffic Classification Using Deep Learning/","link":"","permalink":"https://lulu-cloud.github.io/2022/09/02/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%872%EF%BC%9ADeep%20Packet_%20A%20Novel%20Approach%20For%20Encrypted%20Traffic%20Classification%20Using%20Deep%20Learning/","excerpt":"","text":"0、摘要&emsp;&emsp;论文提出的方案称为“深度包”(deep packet)，可以处理网络流量分类为主要类别(如FTP和P2P)的流量表征，以及需要终端用户应用程序(如BitTorrent和Skype)识别的应用程序识别。与现有的大多数方法不同，深度报文不仅可以识别加密流量，还可以区分VPN网络流量和非VPN网络流量。网络架构基于CNN与SAE，能同时进行应用识别与流量类型的分类任务。 1、介绍&emsp;&emsp;准确的流量分类已成为提供适当的服务质量(quality of service, QoS)、异常检测等高级网络管理任务的先决条件之一。流量分类在与网络管理相关的学术界和工业界都引起了极大的兴趣。 &emsp;&emsp;本文贡献： 在Deep Packet中，不需要专家来提取与网络流量相关的特征。这种方法省去了查找和提取特征的繁琐步骤。（只要是基于DL的方法都能做到这一点） Deep Packet可以在两个粒度级别(应用程序识别和流量表征)上识别流量，并获得最先进的结果。 深度数据包可以准确地分类最难的一类应用程序，已知是P2P。 2、相关工作&emsp;&emsp;等于是一个综述，概览了之前流量分类的一些方法以及它的适用性与优缺点。可以参考论文解读1 Port-based approach（基于端口）：提取过程简单，端口号不受加密方案的影响。然而，端口混淆、网络地址转换(NAT)、端口转发、协议嵌入和端口随机分配的普遍存在大大降低了这种方法的准确性，目前已经不适用。 Payload Inspection Techniques（基于有效载荷）:即深度包检测（DPI）。 Statistical and machine learning approach（基于统计特征+机器学习方法）：这些方法依赖于流量的统计或时间序列特性，能够处理加密和未加密的流量。 &emsp;&emsp;总之，以前的方法，特征提取阶段依赖于人类的特征工程，耗时、昂贵且出错率高。 3、深度学习背景&emsp;&emsp;依旧是综述，关于神经网络的。这里主要介绍了两种神经网络结构。 3.1 自编码器（Autoencoder，AE）&emsp;&emsp;AE是一种无监督框架。考虑一个训练集{x1, x2，…， xn}其中对于每个训练数据我们有xi∈Rn。自编码器目标定义为yi = xi，对于i∈{1,2，…， n}，即网络的输出等于输入。自动编码器试图学习数据集的压缩表示，即将高维数据通过编码器降维，然后降维后的数据通过解码器升维，输出尽量与输入相同。这样。降维后的数据则包含了原始输入数据的信息。一般地，编码器与解码器的结构都是对称的。&emsp;&emsp;在实践中，为了获得更好的性能，一般使用堆栈式自动编码器(SAE)。将多个自动编码器堆叠起来，每个编码器的输出都是连续层的输入，而连续层本身就是一个自动编码器。堆叠式自动编码器的训练过程采用贪婪的分层方式完成。首先，该方法训练网络的每一层，同时冻结其他层的权值。在训练完所有层之后，为了得到更准确的结果，对整个神经网络进行微调。在微调阶段，利用反向传播算法调整各层权重。此外，对于分类任务，可以在最后一层应用额外的softmax层。 3.2 卷积神经网络(Convolutional Neural Network, CNN) 卷积：进行特征抽取 池化：聚合低级特征，获得局部不变性，并且能降低网络训练与测试的参数量。&emsp;&emsp;一维卷积神经网络（1D-CNNs）可以捕获网络数据包中相邻字节之间的空间依赖关系，从而找到每一类协议/应用程序的区别模式，从而对流量进行准确的分类。4、方法4.1 数据集&emsp;&emsp;ISCX VPN-nonVPN：该数据集实在数据链路层捕获的，因此，每个数据包都包含一个以太网报头、一个IP数据报报头、一个TCP/UDP报头。 4.2 预处理 删除以太网报头 将UDP报头填充0至20字节长度（TCP通常具有20字节长度的报头，而UDP具有8字节长度的报头。为了使传输层的段一致，在UDP段的报头末尾注入0，使它们的长度与TCP报头相等） 屏蔽IP数据报报头的IP 删除不相关的数据包，例如没有负载的数据包（TCP握手时SYN、ACK设置为1以及FIN设置为1的数据包）或者DNS数据段（将url转为IP地址的） 将原始数据包转为字节向量 截断超过1500的向量，不足1500长度的填充0 将向量的每个元素除以255来规范化字节向量 针对样本不均衡问题，对样本更多的类进行欠采样，直到类相对平衡。4.3 网络架构 关于SAE的部分：由五个全连接层（FC）,分别由400、300、200、100和50个神经元组成。为防止过拟合问题，每层后采用dropout技术，dropout率为0.05。针对应用识别和流量表征任务，在SAE的最后一层，分别添加了一个包含17个神经元和12个神经元的softmax分类器。 关于CNN的部分：包括两个连续的卷积层，然后是池化层。将二维张量压缩为一维矢量，并将其送入三层全连接神经元网络，该网络采用dropout技术以避免过拟合。最后，将类似于SAE架构的softmax分类器应用于分类任务。CNN的超参数如下：5、实验 对于CNN的调参，此处改变了两个卷积层的滤波器大小、滤波器数量和步幅。总共评估了116个应用识别和交通表征任务的加权平均F1分数模型。通过结果得出如下结论：对于流量分类任务，无法选择最优模型，因为“最优模型”的定义是不明确的，而且模型的精度和它的复杂性(即训练速度和测试速度)之间存在权衡。 增加神经网络的复杂度并不一定会带来更好的性能。可能的原因有：&emsp;&emsp;一个复杂的模型在训练阶段更容易遇到梯度消失问题，从而导致模型的欠拟合。&emsp;&emsp; 一个学习模型变得更复杂，而训练数据的大小保持不变，就会出现过拟合问题。 该工作与Wang W, Zhu M, Wang J, Zeng X, Yang Z (2017)End-to-end encrypted traffic classification with one-dimensional convolution neural networks. In: Intel-ligence and Security Informatics (ISI), 2017 IEEE International Conference on, IEEE. 的方法类似，但Wang等人在流量表征的任务上获得了100%的精度，可能的原因是预处理过程中没有屏蔽IP地址字段，导致模型仅仅用IP地址这一特征来进行分类。 6、总结","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]},{"title":"加密流量分类-论文1：Deep Learning for Encrypted Traffic Classification:An Overview","slug":"加密流量分类-论文1：Deep Learning for Encrypted Traffic Classification_ An Overview","date":"2022-08-30T13:27:14.000Z","updated":"2023-11-24T13:04:17.367Z","comments":true,"path":"2022/08/30/加密流量分类-论文1：Deep Learning for Encrypted Traffic Classification_ An Overview/","link":"","permalink":"https://lulu-cloud.github.io/2022/08/30/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB-%E8%AE%BA%E6%96%871%EF%BC%9ADeep%20Learning%20for%20Encrypted%20Traffic%20Classification_%20An%20Overview/","excerpt":"","text":"0、摘要&emsp;&emsp;这是篇关于加密流量分类的综述，加密流量分类的入门之作，流量分类应用范围广泛，从isp的QoS提供和计费，到防火墙和入侵检测系统的安全应用。从最简单基于端口的、数据包检测到经典的机器学习方法，到由于深度学习的兴起，神经网络成为加密流量分类的主流。本文介绍了常用的深度学习方法及其在流量分类任务中的应用。然后讨论了开放的问题和它们的挑战。关键词：流量分类、深度学习、机器学习 1、介绍简要介绍了加密流量分类方法的发展演变，具体如下：1.1 基于端口（port）的流量分类 优点：简单 缺点：该方法的准确性一直在下降，因为较新的应用程序要么使用众所周知的端口号来掩盖其流量，要么不使用标准的注册端口号。 1.2 基于有效载荷或数据包检验（data packet inspection：DPI）$\\qquad$原理是不同类型的网络流量之中有特定的字符流（也称之为指纹），只需要在数据包的任意位置匹配这些字符流，从而进行分类。 优点：简单快速，只需要检测网络流的前几个数据包。 缺点：方法仅适用于未加密的流量，且计算开销较大。 1.3 基于流量统计特征$\\qquad$这些方法依赖于流量的统计或时间序列特性，能够处理加密和未加密的流量。通常采用经典的机器学习(ML)算法，如随机森林(RF)和k-最近邻(KNN)。 优点：可以用于加密流量，不需解密。 缺点：依赖于人类的特征提取工程，开销大，耗时长。 1.4 基于深度学习方法 优点：深度学习可以通过训练自动选择特征，消除领域专家选择特征的需要，并且具有相当高的学习能力，因此可以学习高度复杂的模式，能够学习原始输入和相应输出之间的非线性关系，而不需要将问题分解为特征选择和分类的小子问题。 缺点：黑盒子，可解释性弱。 2、网络流量分类流程框架 2.1 问题定义&emsp;&emsp;定义分类目标，从哪个领域分类。如nlp中文本分类，一大段关于某个电影的评论文本，可以是情感分类的角度出发分为积极、消极、中性，也可以是影评的类型出发分为关于内容的评论、关于电影演员演技的评论、关于电影启示的评论等等。流量分类分类中，具体的，可以如下： 关于网络协议：流量是何种协议 关于应用程序：流量是何种app中的 关于流量类型：流量是干什么的，如浏览，视频等等 关于网站 关于用户行为：用户是干什么导致此种流量产生 关于浏览器 关于操作系统 还可以分为： 在线分类：要求实时性 离线分类 2.2 数据收集&emsp;&emsp; 对于大多数与流量相关的分类问题，还没有一个公认的数据集。可能的原因包括: 1)可能的流量类别数量巨大，一个数据集几乎不可能包含所有的流量类型; 2)没有普遍接受的数据收集和标记方法; 3)不同的收集方法和场景导致不同的特征可用性和分布。 2.3 数据集预处理 在网络环境中，包重发、重复ack和无序包可能会改变应用程序的流量模式，需要去重。 另一个对深度学习方法性能至关重要的预处理步骤是数据归一化 。在这一步中，所有的输入特征都被缩放到[0,1]范围内的值。使得梯度下降计算时收敛更快，并均衡所有特征的重要性。 2.4 流量特征 时间序列（Time Series）：报文长度、到达时间和连续报文的方向等等。[加密依旧可用] 头部（Header） 负载数据（Payload Data） 统计特征（Statistical Features）：平均包长度、最大包长度、最小到达间隔时间等等。[加密依旧可用] 3 深度学习技术3.1 多层感知机（MLP）一般输入的特征映射至高维，后面逐渐降为到输出类别的维度，层之间一般为全连接（Fully Connected，FC） 3.2 卷积神经网络（CNN）卷积提取特征，池化降低数据量，一般通过若干个卷积与池化操作后，将向量展平通过一到两个FC层后映射到输出类别的维度。CNN的三大特性： 局部连接（感受野） 权重共享（卷积） 空间或时间上的下采样（池化） 一般方法： 一维向量表示一个流（flow）或者会话（session），截取前N个字节数据，作为CNN的输入 将网络流量的时序数据转换为二维图像，作为CNN的输入 3.3 循环神经网络（RNN）在深度学习中，一般会使用RNN的两个变体： 长短期记忆网络（LSTM） 循环门控单元（GRU）RNN擅长处理序列数据，这两种变体解决了传统RNN的梯度消失或者梯度爆炸的问题,&emsp;&emsp;在流量分类上，混合模型（CNN+LSTM）优于纯LSTM或CNN模型。为了同时捕捉流的空间和时间特征，一些研究同时使用了CNN和RNN。 3.4 自编码器（AE） 编码器将输入特征映射到一个低维空间。 解码器的目标则是将此低维向量解码成与原输入相近的特征。 &emsp;&emsp;由此，低维向量中则“蕴含”了原先高维输入的所有信息，达到了抓住高维复杂信息的本质作用。AE是一种无监督学习方法，AE也有许多变体，如去噪自编码器(DAEs)，通过提取损坏的样本来迫使模型学习更健壮的特征，从而输出完整的输入样本。变分自编码器(VAEs)，旨在从目标分布生成虚拟样本。可以叠加深层的，称为堆叠自动编码器(SAE) 3.5 对抗生成网络（GAN） &emsp;&emsp;GAN是一种无监督技术，它同时训练生成模型和判别模型。 生成器（Generator）旨在生成目标分布的(假)样例，“骗过”鉴别器。输入一般为高斯分布的随机噪声，输出一个生成伪样本。 鉴别器（Discriminator）模型旨在区分真实数据和生成数据。输入为真实样本与伪样本，判定真是样本与伪样本的真实性，如果鉴别器无力辨别真假，则模型可能收敛。&emsp;&emsp;这两个模型通常都是神经网络。首先通过鉴别器训练生成器，使其误差概率最大化。然后，固定生成器，训练鉴别器，使输入真实数据和生成数据的误差概率最小。这个过程一直持续到它收敛为止。 &emsp;&emsp;生成模型可用于处理网络流量分类中数据集的不平衡问题。不平衡问题指的是每个类别的样本数量差异很大的情况。 3.6 模型选择 特征选择直接影响神经网络输入结构和维度，这些维度影响计算复杂度和用于分类的数据包数量(内存复杂度)。 根据所选择的特征选择合适的模型 3.6.1 时间序列（Time Series）+报头（Header）&emsp;时间序列特性几乎不受加密的影响，基本经典的ML算法与MLP都可以适用，当然CNN等模型可以获得更好的效益与更好的结果。 3.6.2 负载（playload）+报头（Header）&emsp;在当前加密的通信中，前几个包含握手信息的包通常是未加密的，它们已经成功地用于分类。但是负载中包含大量字节，导致数据维度较高，传统的ML与MLP不能很好适用，一般使用CNN或者CNN与LSTM结合的模型进行流量分类。 3.6.3 统计特征（Statistical Features）&emsp;统计特征的数量是有限的，因此输入维度也是有限的。因此，大多数论文对这些特征使用了经典的ML方法，或在少数情况下使用MLP方法。&emsp;虽然大多数研究通过观察整个流程来获得统计特征，但研究表明，根据数据集和统计特征的选择，从前10到180个数据包中获得统计特征可能足以进行分类。不适合在线快速分类，因为它需要捕获足够多的数据包来从一个流中获得可靠的统计特征。 3.7 训练与验证&emsp;&emsp;通常，数据集被分为三个独立的集:训练集、验证集和测试集。该模型在训练集上进行训练，通过观察验证集的准确性来调整模型的超参数。最后利用测试集得到了无偏精度。&emsp;&emsp;简言之，训练集上进行梯度下降更新模型参数，验证集上选择最优超参数组合，测试集上进心模型评估。 3.8 定期评估/更新&emsp;&emsp;在大多数与网络相关的应用程序中，流量在变化，依此流量特性总是在变化的。因此需要更新模型，增加模型分类普适性、健壮性。 4 开放机遇与挑战4.1 更强大的加密协议&emsp;&emsp;例如关于QUIC和TLS 1.3协议的流量分类，还没有得到很好的研究。以往对TLS 1.2的研究主要使用握手时的纯文本字段。但是，通过在TLS 1.3和QUIC中引入0-RTT连接，第一个包中只有少数字段保持未加密，不清楚它们是否足以用于分类。 4.2 多标签分类&emsp;&emsp;单个流可以包含多个类标签，称为多路复用流。最困难的挑战是如何适当地收集和标记这些多标签流量。 4.3 中间流分类？（Middle Flow Classification）&emsp;&emsp;目前大部分的流量分类就是基于流的前几个数据包，因此，ISP需要存储所有流的前几个数据包，空间负担重。如果可以基于流中间的几个数据包，那么ISP就可以等待并且检测大象流（elephant flow），然后捕获流中间的几个数据包进行分类，能减少内存和计算开销。 关于大象流、老鼠流： 大象流（elephant flow）：大象流是通过网络链路进行大量的，持续的传递数据的过程。 老鼠流（mouse flow）：老鼠流是通过网络链路进行少量的，短时间的数据传递过程。 &emsp;发邮件，看网页，聊微信，这些都属于老鼠流。而虚机的迁移，数据的迁移，MapReduce等等，属于大象流。 4.4 零日应用问题（Zero-day Application）&emsp;零日应用是指新的流量类，它们的样本不存在于训练集中。已经表明，在某些情况下，零日应用程序可以在网络流量中占60%的流量和30%的字节。最近只有少数[14]研究提出了解决方案，通常依赖于检测未标记的簇，然后对它们进行标记。在ML社区中，主动学习(由模型选择哪些数据点应该被标记)已经被研究了多年。（聚类算法） 在最近一项对图像分类的研究中，强化学习和LSTM的结合被用于执行两种可能的操作之一:预测类别或要求一个新标签。 4.5 迁移学习与领域自适应&emsp;&emsp;收集到足够大的代表性数据集是不容易的，通常更容易获得为其他任务捕获的大型数据集，这可能有助于模型提取公共特征。此外，训练一个深度模型通常耗费时间长，而对模型进行再训练通常收敛更快，因此最好是对已经为类似任务进行过训练的模型进行再训练。 迁移学习允许在源任务上训练的模型用于不同的目标任务，并且假设源任务与目标任务输入分布相似。 领域自适应处理的是任务相同，但源和目标的输入分布不同的情况。 4.6 多任务学习&emsp;&emsp;一个模型中有一个以上的损失函数被优化。研究表明，即使对于单任务问题，增加一些辅助任务也可以提高泛化能力和性能。然而，对于网络流量分类任务还没有进行过研究。可能有很多方法可以定义辅助任务，而不需要额外的标记。多任务学习在网络流量分类中的有效性还没有得到研究。 第一次写论文博客，很水，写给自己看顺便总结一下，如有理解错误的地方，谢谢指正。 参考：[1].S. Rezaei and X. Liu, “Deep Learning for Encrypted Traffic Classification: An Overview,” in IEEE Communications Magazine, vol. 57, no. 5, pp. 76-81, May 2019, doi: 10.1109/MCOM.2019.1800819.[2].大象流老鼠流的解释：https://www.zhihu.com/question/50171430[3].王茂南. 基于深度学习的加密流量识别技术研究[D].北京邮电大学,2021.DOI:10.26969/d.cnki.gbydu.2021.000480.","categories":[{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"}]}],"categories":[{"name":"硕士生活","slug":"硕士生活","permalink":"https://lulu-cloud.github.io/categories/%E7%A1%95%E5%A3%AB%E7%94%9F%E6%B4%BB/"},{"name":"科研","slug":"科研","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/"},{"name":"加密流量分类","slug":"科研/加密流量分类","permalink":"https://lulu-cloud.github.io/categories/%E7%A7%91%E7%A0%94/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"生活随记","slug":"生活随记","permalink":"https://lulu-cloud.github.io/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E8%AE%B0/"},{"name":"论文","slug":"论文","permalink":"https://lulu-cloud.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"实验","slug":"实验","permalink":"https://lulu-cloud.github.io/tags/%E5%AE%9E%E9%AA%8C/"}]}